{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Titanic Survival Prediction Using Neural Networks\n",
        "\n",
        "This lab focuses on building and training a neural network model to predict survival on the Titanic."
      ],
      "metadata": {
        "id": "thWXAS-2Vghs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Titanic Dataset"
      ],
      "metadata": {
        "id": "mQEozfJs65mG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kqb6M6AXAXio"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "root_dir = \"PATH/TO/YOUR/DIRECTORY\"\n",
        "\n",
        "# Checking if our specified directory exists\n",
        "os.path.exists(root_dir)"
      ],
      "metadata": {
        "id": "as6DDQKbAdpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to the downloaded files\n",
        "data_path = os.path.join(root_dir, \"titanic_train.csv\")\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(data_path)\n",
        "df"
      ],
      "metadata": {
        "id": "eRrp3nRBw61L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 100\n",
        "target = \"Survived\""
      ],
      "metadata": {
        "id": "7VpQTmeOI62J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "lDyEIdkO71Eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "c2SpUcmU8IRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variable Selection\n",
        "\n",
        "Eliminate variables that are not utilized as inputs or that contain numerous missing values."
      ],
      "metadata": {
        "id": "xAe7ABA_8WKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_vars = [\"Name\", \"PassengerId\", \"Ticket\", \"Cabin\"]\n",
        "df.drop(drop_vars, axis=1, inplace=True)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "Zgq5hr6JeacN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Value Imputation\n",
        "\n",
        "* [sklearn.impute.SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer): Univariate imputer for completing missing values with simple strategies.\n",
        "* [sklearn.impute.KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer): Imputation for completing missing values using k-Nearest Neighbors. Each sample’s missing values are imputed using the mean value from `n_neighbors` nearest neighbors found in the training set.\n",
        "* [sklearn.impute.IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer): Multivariate imputer that estimates each feature from all the others. A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion. (Default estimator: `BayesianRidge`)"
      ],
      "metadata": {
        "id": "hgxIzKqI8cd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
        "\n",
        "df_imputed = df.copy()\n",
        "\n",
        "# Mode imputation\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "df_imputed[['Embarked']] = imputer.fit_transform(df[['Embarked']])\n",
        "\n",
        "\n",
        "features = ['Age', 'Pclass', 'SibSp', 'Parch']  # Ensure all features are numerical\n",
        "\n",
        "# # K-Nearest Neighbors (KNN) Imputation\n",
        "# imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# Multivariate Imputation by Chained Equations (MICE)\n",
        "imputer = IterativeImputer()\n",
        "\n",
        "# # Random Forest Imputation\n",
        "# imputer = IterativeImputer(estimator=RandomForestRegressor())\n",
        "\n",
        "df_imputed[features] = imputer.fit_transform(df[features])"
      ],
      "metadata": {
        "id": "1ggyKksSR1ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(df_imputed['Age'], kde=True, color='blue', alpha=0.5, label='Imputed Age')\n",
        "sns.histplot(df['Age'].dropna(), kde=True, color='red', alpha=0.5, label='Original Age')\n",
        "plt.legend()\n",
        "plt.title('Distribution of Age Before and After Imputation')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t-Y4uy22WgAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_imputed"
      ],
      "metadata": {
        "id": "9aAPruQFnsMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Categorical Variables"
      ],
      "metadata": {
        "id": "x5jkjJ0j80Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "VS8rxV-Efz3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Sex\"] = df[\"Sex\"].replace({\"male\": 0, \"female\": 1})\n",
        "\n",
        "var = \"Embarked\"\n",
        "one_hot = pd.get_dummies(df[var], prefix=var)\n",
        "df = pd.concat([df, one_hot], axis=1).drop([var], axis=1)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "JKQR7q89gWFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = df.drop(target, axis=1).columns\n",
        "features"
      ],
      "metadata": {
        "id": "IIE7I0HYM-j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outlier Detection\n",
        "\n",
        "* Using Z-score, Interquartile Range (IQR)\n",
        "* [sklearn.ensemble.IsolationForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html): The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n",
        "* [sklearn.cluster.DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html): Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Noisy samples are given the label -1."
      ],
      "metadata": {
        "id": "Qat1hDjvTMUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IsolationForest"
      ],
      "metadata": {
        "id": "J1SHvQVzZWeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "df_outlier = df.copy()\n",
        "\n",
        "iso_forest = IsolationForest(n_estimators=100, contamination=0.02, random_state=random_state)\n",
        "outliers = iso_forest.fit_predict(df_outlier[features])\n",
        "\n",
        "print(\"Outliers detected:\", np.sum(outliers == -1))\n",
        "df_outlier['outlier'] = outliers\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Age', y='Fare', hue='outlier', data=df_outlier, palette={-1: 'red', 1: 'blue'})\n",
        "plt.title('Outlier Detection with Isolation Forest')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Fare')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vuuZbl4THgIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_outlier[features])\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "\n",
        "colors = {1: 'blue', -1: 'red'}\n",
        "marker_colors = [colors[label] for label in df_outlier['outlier']]\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=marker_colors, marker='o')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Plot with Outliers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gKlA8c5oIBxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN\n",
        "\n",
        "<img src=\"https://machinelearninggeek.com/wp-content/uploads/2020/10/image-58.png\" width=\"800\">"
      ],
      "metadata": {
        "id": "PpjwHkeTZcU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "df_outlier = df.copy()\n",
        "\n",
        "dbscan = DBSCAN(eps=1.0, min_samples=5)\n",
        "clusters = dbscan.fit_predict(X_pca)\n",
        "\n",
        "outliers = np.sum(clusters == -1)\n",
        "print(\"Number of outliers:\", outliers)\n",
        "df_outlier['outlier'] = np.where(clusters == -1, -1, 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Age', y='Fare', hue='outlier', data=df_outlier, palette={-1: 'red', 1: 'blue'})\n",
        "plt.title('Outlier Detection with DBSCAN Clustering')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Fare')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "725FkZ77MIwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10, 8))\n",
        "\n",
        "colors = {1: 'blue', -1: 'red'}\n",
        "marker_colors = [colors[label] for label in df_outlier['outlier']]\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=marker_colors, marker='o')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Plot with Outliers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VXYHiFBeSTKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Split\n",
        "\n",
        "Split the data into training and test sets."
      ],
      "metadata": {
        "id": "t92neXe7ADM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "shuffle = True\n",
        "test_size_ratio = 0.25\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=test_size_ratio, random_state=random_state, shuffle=shuffle)\n",
        "print(train_df.shape, test_df.shape)"
      ],
      "metadata": {
        "id": "RgVcEmuTdIk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df.drop(target, axis=1).values\n",
        "y_train = train_df[target].values\n",
        "\n",
        "X_test = test_df.drop(target, axis=1).values\n",
        "y_test = test_df[target].values"
      ],
      "metadata": {
        "id": "yMzd_Ig8--j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Normalization\n",
        "\n",
        "Utilizes [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from sklearn to normalize the training and testing datasets."
      ],
      "metadata": {
        "id": "j4Bwap9EKpFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "OokpkV3EKnWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation using Scikit-Learn"
      ],
      "metadata": {
        "id": "B_p773qhGNnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "* MLP Classifier ([sklearn.neural_network.MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html))"
      ],
      "metadata": {
        "id": "dury0KLRLhSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "model = MLPClassifier(hidden_layer_sizes=(50, 30),\n",
        "                      max_iter=300,\n",
        "                      activation='relu',\n",
        "                      solver='adam',\n",
        "                      batch_size=200,\n",
        "                      learning_rate='invscaling',\n",
        "                      learning_rate_init=0.01,\n",
        "                      power_t=0.5,  # Exponent for inverse scaling learning rate\n",
        "                      warm_start=True,\n",
        "                      random_state=random_state,\n",
        "                      verbose=True) # Enable verbose to monitor\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Access the loss_curve_ attribute\n",
        "loss_values = model.loss_curve_\n",
        "\n",
        "# Plot the loss curve\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(loss_values, label='Loss per iteration')\n",
        "plt.title('Training Loss per Iteration')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RfKRnBuqPxdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "u2PWejcLcTBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob = model.predict_proba(X_test)\n",
        "print(\"Estimated probs:\", y_prob[:10])\n",
        "\n",
        "y_cls = model.predict(X_test)\n",
        "print(\"Estimated classes:\", y_cls[:10])\n",
        "print()"
      ],
      "metadata": {
        "id": "iuX2-W6JawUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Accuracy ([metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html))\n",
        "* F1 ([metrics.f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html))\n",
        "* ROC AUC ([metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html))"
      ],
      "metadata": {
        "id": "YMuRuYl8ceZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_cls))\n",
        "print(\"F1:\", f1_score(y_test, y_cls))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob[:, 1]))"
      ],
      "metadata": {
        "id": "ipVtYkQJcV65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Confusion Matrix ([metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html))"
      ],
      "metadata": {
        "id": "CyLZj3_Ych5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_cls)\n",
        "conf_matrix_df = pd.DataFrame(\n",
        "    conf_matrix,\n",
        "    columns=[\"Predicted Not-Survived\", \"Predicted Survived\"],\n",
        "    index=[\"Actual Not-Survived\", \"Actual Survived\"]\n",
        ")\n",
        "print(conf_matrix_df)"
      ],
      "metadata": {
        "id": "5t_v8WGrcZvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ROC Curve ([metrics.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html))"
      ],
      "metadata": {
        "id": "FR8VvBfQclES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
        "\n",
        "plt.plot(fpr, tpr, color=\"darkorange\", lw=2)\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
        "plt.xlabel(\"1 - Specificity (FP Rate)\")\n",
        "plt.ylabel(\"Sensitivity (TP Rate)\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R19hkYuCccjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R35OlZ8eHH9c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}