{"cells":[{"cell_type":"markdown","metadata":{"id":"X74omqNkT6IP"},"source":["# Dataset과 DataLoader\n"]},{"cell_type":"markdown","metadata":{"id":"bzszilVOT6IP"},"source":["데이터 샘플을 처리하는 코드는 지저분하고 유지보수가 어려울 수 있습니다.\n","더 나은 가독성(readability)과 모듈성(modularity)을 위해 데이터셋 코드를 모델 학습 코드로부터 분리하는 것이 이상적입니다.\n","PyTorch는 ``torch.utils.data.DataLoader``와 ``torch.utils.data.Dataset``의 두 가지 데이터 기본 요소를 제공하여 미리 준비되어 있는 데이터셋 뿐만 아니라 가지고 있는 데이터를 사용할 수 있도록 합니다.\n","``Dataset``은 샘플과 정답(label)을 저장하고, ``DataLoader``는 ``Dataset``을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감쌉니다.\n","\n","PyTorch의 도메인 특화 라이브러리들은 (FashionMNIST와 같은) 미리 준비되어 있는 다양한 데이터셋을 제공합니다.\n","데이터셋은 ``torch.utils.data.Dataset``의 하위 클래스로 개별 데이터를 특정하는 함수가 구현되어 있습니다.\n","이러한 데이터셋은 모델을 만들어보고(prototype) 성능을 측정(benchmark)하는데 사용할 수 있습니다.\n","여기에서 데이터셋들을 찾아볼 수 있습니다:\n","[이미지 데이터셋](https://pytorch.org/vision/stable/datasets.html),\n","[텍스트 데이터셋](https://pytorch.org/text/stable/datasets.html) 및\n","[오디오 데이터셋](https://pytorch.org/audio/stable/datasets.html)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NWzaT5nnT6IP"},"source":["## 데이터셋 불러오기\n","\n","`TorchVision` 에서 [Fashion-MNIST 데이터셋](https://github.com/zalandoresearch/fashion-mnist)을\n","불러오는 예제를 살펴보겠습니다. Fashion-MNIST는 Zalando의 기사 이미지 데이터셋으로 60,000개의 학습 예제와 10,000개의 테스트 예제로 이루어져 있습니다.\n","각 예제는 흑백(grayscale)의 28x28 이미지와 10개 클래스 중 하나인 정답으로 구성됩니다.\n","\n","다음 매개변수들을 사용하여 [FashionMNIST 데이터셋](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST) 을 불러옵니다:\n"," - ``root``는 학습/테스트 데이터가 저장되는 경로입니다.\n"," - ``train``은 학습/테스트 데이터 여부를 지정합니다.\n"," - ``download=True``는 ``root``에 데이터가 없는 경우 인터넷에서 다운로드합니다.\n"," - ``transform``과 ``target_transform``은 특징(feature)과 정답(label) 변형(transform)을 지정합니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vROt2GzWT6IQ"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","\n","\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")"]},{"cell_type":"markdown","metadata":{"id":"LT8JiI5XT6IQ"},"source":["## 데이터셋을 순회하고 시각화하기\n","\n","``Dataset``에 리스트(list)처럼 직접 접근(index)할 수 있습니다: ``training_data[index]``.\n","``matplotlib``을 사용하여 학습 데이터의 일부를 시각화해보겠습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JjY84s-IT6IQ"},"outputs":[],"source":["labels_map = {\n","    0: \"T-Shirt\",\n","    1: \"Trouser\",\n","    2: \"Pullover\",\n","    3: \"Dress\",\n","    4: \"Coat\",\n","    5: \"Sandal\",\n","    6: \"Shirt\",\n","    7: \"Sneaker\",\n","    8: \"Bag\",\n","    9: \"Ankle Boot\",\n","}\n","\n","figure = plt.figure(figsize=(8, 8))\n","cols, rows = 3, 3\n","\n","for i in range(1, cols * rows + 1):\n","    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n","    img, label = training_data[sample_idx]\n","    figure.add_subplot(rows, cols, i)\n","    plt.title(labels_map[label])\n","    plt.axis(\"off\")\n","    plt.imshow(img.squeeze(), cmap=\"gray\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"AmVly0UZT6IQ"},"source":["------------------------------------------------------------------------------------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8257rhZlT6IQ"},"source":["## 사용자 정의 데이터셋 만들기\n","\n","사용자 정의 Dataset 클래스는 반드시 3개 함수를 구현해야 합니다: `__init__`, `__len__`, and `__getitem__`.\n","아래 구현을 살펴보면 FashionMNIST 이미지들은 ``img_dir`` 디렉토리에 저장되고, 정답은 ``annotations_file`` csv 파일에\n","별도로 저장됩니다.\n","\n","다음 장에서 각 함수들에서 일어나는 일들을 자세히 살펴보겠습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxJpE5_JT6IQ"},"outputs":[],"source":["import os\n","import pandas as pd\n","from torchvision.io import read_image\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","        self.img_labels = pd.read_csv(annotations_file, names=['file_name', 'label'])\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = read_image(img_path)\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        sample = {\"image\": image, \"label\": label}\n","        return sample"]},{"cell_type":"markdown","metadata":{"id":"avWOv7sAT6IQ"},"source":["### __init__\n","\n","__init__ 함수는 Dataset 객체가 생성(instantiate)될 때 한 번만 실행됩니다.\n","여기서는 이미지와 주석 파일(annotation_file)이 포함된 디렉토리와 두가지 변형(transform)을 초기화합니다.\n","\n","labels.csv 파일은 다음과 같습니다:\n","\n","    tshirt1.jpg, 0\n","    tshirt2.jpg, 0\n","    ......\n","    ankleboot999.jpg, 9\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7SR-sCwT6IQ"},"outputs":[],"source":["def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","    self.img_labels = pd.read_csv(annotations_file, names=['file_name', 'label'])\n","    self.img_dir = img_dir\n","    self.transform = transform\n","    self.target_transform = target_transform"]},{"cell_type":"markdown","metadata":{"id":"NuEWMt6ZT6IQ"},"source":["### __len__\n","\n","__len__ 함수는 데이터셋의 샘플 개수를 반환합니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTlKrmJnT6IR"},"outputs":[],"source":["def __len__(self):\n","    return len(self.img_labels)"]},{"cell_type":"markdown","metadata":{"id":"yB4GBHDKT6IR"},"source":["### __getitem__\n","\n","__getitem__ 함수는 주어진 인덱스 ``idx``에 해당하는 샘플을 데이터셋에서 불러오고 반환합니다.\n","인덱스를 기반으로, 디스크에서 이미지의 위치를 식별하고, ``read_image``를 사용하여 이미지를 텐서로 변환하고, ``self.img_labels`` 의 csv 데이터로부터\n","해당하는 정답(label)을 가져오고, (해당하는 경우) 변형(transform) 함수들을 호출한 뒤, 텐서 이미지와 라벨을 Python 사전(dict)형으로 반환합니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-QSaVwYT6IR"},"outputs":[],"source":["def __getitem__(self, idx):\n","    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","    image = read_image(img_path)\n","    label = self.img_labels.iloc[idx, 1]\n","    if self.transform:\n","        image = self.transform(image)\n","    if self.target_transform:\n","        label = self.target_transform(label)\n","    # return image, label\n","    sample = {\"image\": image, \"label\": label}\n","    return sample"]},{"cell_type":"markdown","metadata":{"id":"hYu_H2nvT6IR"},"source":["------------------------------------------------------------------------------------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-3VmnKlET6IR"},"source":["## DataLoader로 학습용 데이터 준비하기\n","\n","``Dataset``은 특정 데이터 샘플에 대한 특징(feature)과 정답(label)을 지정하는 일을 한 번에 합니다.\n","모델을 학습할 때, 일반적으로 샘플들을 \"미니배치(minibatch)\"로 전달하고, 매 에폭(epoch)마다 데이터를 다시 섞어서 과적합(overfit)을 막습니다.\n","\n","``DataLoader``는 간단한 API로 이러한 복잡한 과정들을 추상화한 순회 가능한 객체(iterable)입니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jSQdHXc3T6IR"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"OTnNG4wQT6IR"},"source":["## DataLoader를 통해 순회하기(iterate)\n","\n","``DataLoader``에 데이터셋을 불러온 뒤에는 필요에 따라 데이터셋을 순회(iterate)할 수 있습니다.\n","아래의 각 순회(iteration)는 (각각 ``batch_size=64`` 의 입력(image)과 정답(label)을 포함하는) ``train_images`` 와\n","``train_labels`` 의 묶음(batch)을 반환합니다. ``shuffle=True`` 로 지정했으므로, 모든 배치를 순회한 뒤 데이터가 섞입니다.\n","(데이터 불러오기 순서를 보다 세밀하게 제어하려면 [Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)를 살펴보세요.)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r5_gtQhVT6IR"},"outputs":[],"source":["train_images, train_labels = next(iter(train_dataloader))\n","print(f\"Images batch shape: {train_images.size()}\")\n","print(f\"Labels batch shape: {train_labels.size()}\")\n","img = train_images[0].squeeze()\n","label = train_labels[0]\n","plt.imshow(img, cmap=\"gray\")\n","plt.show()\n","print(f\"Label: {label}\")"]},{"cell_type":"markdown","metadata":{"id":"Uma4B4nOT6IR"},"source":["------------------------------------------------------------------------------------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XkjySfbnT6IR"},"source":["## 더 읽어보기\n","- [torch.utils.data API](https://pytorch.org/docs/stable/data.html)\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}