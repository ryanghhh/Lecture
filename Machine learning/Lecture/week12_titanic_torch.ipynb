{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Titanic Survival Prediction Using PyTorch\n","\n","This lab focuses on building and training a neural network model to predict survival on the Titanic. The session will guide you through the process of handling a real-world tabular dataset, preprocessing it, and applying a machine learning model using PyTorch."],"metadata":{"id":"thWXAS-2Vghs"}},{"cell_type":"code","source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"-m4C8anaJ9cu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Titanic Dataset"],"metadata":{"id":"mQEozfJs65mG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kqb6M6AXAXio"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","source":["import os\n","\n","# root_dir = \"PATH/TO/YOUR/DIRECTORY\"\n","root_dir = \"/content/gdrive/MyDrive/lecture/[Common] 머신러닝 원리와 응용/lab12_nn_torch\"\n","\n","# Checking if our specified directory exists\n","os.path.exists(root_dir)"],"metadata":{"id":"as6DDQKbAdpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Paths to the downloaded files\n","data_path = os.path.join(root_dir, \"titanic_train.csv\")\n","\n","# Load data\n","df = pd.read_csv(data_path)\n","df"],"metadata":{"id":"eRrp3nRBw61L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random_state = 100\n","target = \"Survived\""],"metadata":{"id":"7VpQTmeOI62J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Preprocessing"],"metadata":{"id":"lDyEIdkO71Eg"}},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"c2SpUcmU8IRq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variable Selection\n","\n","Eliminate variables that are not utilized as inputs or that contain numerous missing values."],"metadata":{"id":"xAe7ABA_8WKr"}},{"cell_type":"code","source":["drop_vars = [\"Name\", \"PassengerId\", \"Ticket\", \"Cabin\"]\n","df.drop(drop_vars, axis=1, inplace=True)\n","df.info()"],"metadata":{"id":"Zgq5hr6JeacN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Missing Value Imputation\n","\n","* [sklearn.impute.SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer): Univariate imputer for completing missing values with simple strategies.\n","* [sklearn.impute.KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer): Imputation for completing missing values using k-Nearest Neighbors. Each sample’s missing values are imputed using the mean value from `n_neighbors` nearest neighbors found in the training set.\n","* [sklearn.impute.IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer): Multivariate imputer that estimates each feature from all the others. A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion. (Default estimator: `BayesianRidge`)"],"metadata":{"id":"hgxIzKqI8cd2"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n","\n","df_imputed = df.copy()\n","\n","# Mode imputation\n","imputer = SimpleImputer(strategy='most_frequent')\n","df_imputed[['Embarked']] = imputer.fit_transform(df[['Embarked']])\n","\n","\n","features = ['Age', 'Pclass', 'SibSp', 'Parch']  # Ensure all features are numerical\n","\n","# # K-Nearest Neighbors (KNN) Imputation\n","# imputer = KNNImputer(n_neighbors=5)\n","\n","# Multivariate Imputation by Chained Equations (MICE)\n","imputer = IterativeImputer()\n","\n","# # Random Forest Imputation\n","# imputer = IterativeImputer(estimator=RandomForestRegressor())\n","\n","df_imputed[features] = imputer.fit_transform(df[features])\n","\n","df = df_imputed"],"metadata":{"id":"1ggyKksSR1ST"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Handling Categorical Variables"],"metadata":{"id":"x5jkjJ0j80Oj"}},{"cell_type":"code","source":["df"],"metadata":{"id":"VS8rxV-Efz3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[\"Sex\"] = df[\"Sex\"].replace({\"male\": 0, \"female\": 1})\n","\n","var = \"Embarked\"\n","one_hot = pd.get_dummies(df[var], prefix=var)\n","df = pd.concat([df, one_hot], axis=1).drop([var], axis=1)\n","\n","df"],"metadata":{"id":"JKQR7q89gWFh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = df.drop(target, axis=1).columns\n","features"],"metadata":{"id":"IIE7I0HYM-j6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Split\n","\n","Split the data into training and test sets."],"metadata":{"id":"t92neXe7ADM0"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","shuffle = True\n","test_size_ratio = 0.25\n","\n","train_df, test_df = train_test_split(df, test_size=test_size_ratio, random_state=random_state, shuffle=shuffle)\n","print(train_df.shape, test_df.shape)"],"metadata":{"id":"RgVcEmuTdIk-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = train_df.drop(target, axis=1).values\n","y_train = train_df[target].values\n","\n","X_test = test_df.drop(target, axis=1).values\n","y_test = test_df[target].values"],"metadata":{"id":"yMzd_Ig8--j2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Normalization\n","\n","Utilizes [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from sklearn to normalize the training and testing datasets."],"metadata":{"id":"j4Bwap9EKpFX"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","# Normalize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"OokpkV3EKnWV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Over-sampling\n","\n","- In this dataset, the number of **survivors** is significantly smaller compared to **non-survivors**, leading to a potential imbalance that can cause a model to be biased towards the majority class (non-survivors).\n","- **[SMOTE (Synthetic Minority Over-sampling Technique)](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)** is an **oversampling technique** used to address class imbalance by **synthesizing new samples** of the minority class rather than simply duplicating existing ones.\n","- It works by selecting instances from the minority class and then creating new synthetic examples along the lines between the selected instance and one of its **k-nearest neighbors**. This helps the model learn better and avoids the pitfalls of overfitting to duplicate data."],"metadata":{"id":"rVzS6iruFKOG"}},{"cell_type":"code","source":["from imblearn.over_sampling import SMOTE\n","\n","smote = SMOTE(sampling_strategy='auto', random_state=random_state, k_neighbors=5)\n","X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"],"metadata":{"id":"ud17ehaLFJYv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check class ratio before/after applying SMOTE"],"metadata":{"id":"LDcygqNpH1V1"}},{"cell_type":"code","source":["print(\"Class distribution in y_train before SMOTE:\")\n","print(pd.Series(y_train).value_counts())\n","\n","print(\"Class distribution in y_train after SMOTE:\")\n","print(pd.Series(y_train_smote).value_counts())"],"metadata":{"id":"QhcGxhJtHNsA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, y_train = X_train_smote, y_train_smote"],"metadata":{"id":"6VfCGu84Huvs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training and Evaluation using PyTorch"],"metadata":{"id":"ncl8QbsEGZTJ"}},{"cell_type":"markdown","source":["### Preparation for PyTorch Training\n","\n","Conversion to PyTorch Tensors:\n","- The normalized data is then converted into PyTorch tensors, which are the fundamental data structures used in PyTorch for building and training neural networks.\n","- `FloatTensor` is used for input features (`X_train` and `X_test`), and `LongTensor` for labels (`y_train` and `y_test`), matching the data types expected by PyTorch models.\n","\n","Creating DataLoaders:\n","- `TensorDataset` wraps tensors into a dataset. Each sample will be retrieved by indexing tensors along the first dimension.\n","- `DataLoader` is used to create iterable over the datasets. `train_loader` and `test_loader` are created with specified batch sizes and shuffling options."],"metadata":{"id":"Qp3LZ7dNGddn"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, TensorDataset\n","\n","batch_size = 200\n","\n","# Convert to PyTorch tensors\n","X_train = torch.FloatTensor(X_train)\n","X_test = torch.FloatTensor(X_test)\n","y_train = torch.LongTensor(y_train)\n","y_test = torch.LongTensor(y_test)\n","\n","# Create dataloaders\n","train_dataset = TensorDataset(X_train, y_train)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","\n","test_dataset = TensorDataset(X_test, y_test)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"KuIRckBjcjqG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Architecture\n","\n","The `SimpleNN` class extends PyTorch's `nn.Module` and represents a simple fully connected neural network (also known as a Multilayer Perceptron) with two hidden layers."],"metadata":{"id":"_iEEPqx9GlzK"}},{"cell_type":"code","source":["class SimpleNN(nn.Module):\n","    def __init__(self, hidden_sizes=(50, 30), apply_bn=False):\n","        super(SimpleNN, self).__init__()\n","        self.fc1 = nn.Linear(len(features), hidden_sizes[0])\n","        self.bn1 = nn.BatchNorm1d(hidden_sizes[0])\n","        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n","        self.bn2 = nn.BatchNorm1d(hidden_sizes[1])\n","        self.fc3 = nn.Linear(hidden_sizes[1], 2)\n","        self.apply_bn = apply_bn\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        if self.apply_bn:\n","            x = self.bn1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        if self.apply_bn:\n","            x = self.bn2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","        return x\n","\n","model = SimpleNN(hidden_sizes=(50, 30), apply_bn=True)\n","model"],"metadata":{"id":"nmiA-iRWGfqL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Weight Initialization (Skipped)\n","\n","In PyTorch, the default weight initialization method varies depending on the type of layer in the neural network. By default, PyTorch initializes the weights of `nn.Linear` layers using Kaiming uniform (He uniform) initialization, and the biases are set to zero. This is a common choice for layers that are followed by non-linear activations like ReLU."],"metadata":{"id":"VhmU6tGfGxJw"}},{"cell_type":"markdown","source":["### Optimizer and Learning Rate Scheduler\n","\n","When training a neural network, choosing the right optimizer and regularization technique can significantly impact performance. In the code snippet provided, we define a method to select an optimizer based on predefined hyperparameters and apply L2 regularization to prevent overfitting."],"metadata":{"id":"hWB0i0d4Gzk-"}},{"cell_type":"code","source":["from torch.optim import Adam, SGD, RMSprop\n","from torch.optim.lr_scheduler import StepLR\n","\n","def select_optimizer(optimizer_name, parameters, lr=1e-3, weight_decay=0):\n","    if optimizer_name == \"sgd\":\n","        return torch.optim.SGD(parameters, lr=lr, weight_decay=weight_decay, momentum=0.9)\n","    elif optimizer_name == \"rmsprop\":\n","        return torch.optim.RMSprop(parameters, lr=lr, weight_decay=weight_decay, alpha=0.99)\n","    elif optimizer_name == \"adam\":\n","        return torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n","    else:\n","        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n","\n","\n","# Choose optimizer and regularization hyperparameters\n","optimizer_name = \"adam\" # Could be \"sgd\", \"rmsprop\", or \"adam\"\n","learning_rate = 0.001\n","weight_decay = 0.001    # L2 regularization coefficient\n","\n","optimizer = select_optimizer(optimizer_name=optimizer_name, parameters=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","scheduler = StepLR(optimizer, step_size=50, gamma=0.1)"],"metadata":{"id":"q1yzLIiAGvPH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training\n","\n","This section of the code represents the training loop for our neural network model. The loop iterates over the dataset multiple times (epochs), updating the model's weights to minimize the loss function, which in this case measures the discrepancy between the predicted and actual class labels."],"metadata":{"id":"ZQgqp9WOG_VB"}},{"cell_type":"code","source":["num_epochs = 100\n","train_losses = []\n","\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        # Clear old gradients; if not cleared, they would accumulate with subsequent backward passes.\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs) # Forward pass to get predictions.\n","        loss = F.cross_entropy(outputs, labels, reduction='mean') # Use mean for gradient calculation\n","        loss.backward()         # Backpropagation to compute the gradients.\n","        running_loss += loss.item() * inputs.size(0)              # Use sum for tracking running loss\n","\n","        # Update the weights of the model based on the gradients calculated during backpropagation.\n","        optimizer.step()\n","\n","        # train_losses.append(loss.item())\n","\n","    average_loss = running_loss / len(train_loader)\n","    current_lr = scheduler.get_last_lr()[0]\n","    print(f\"[Epoch {epoch + 1}] (LR: {current_lr:.8f}) Average Loss: {average_loss:.4f}\")\n","\n","    # Store the loss for visualization\n","    train_losses.append(average_loss)\n","\n","    # Update the learning rate according to the specified schedule\n","    scheduler.step()\n","\n","print(\"Finished Training\")"],"metadata":{"id":"jVymsuJUG62a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Plot the training loss\n","plt.figure(figsize=(10, 6))\n","plt.plot(train_losses, label='Training Loss')\n","plt.xlabel('Step')\n","plt.ylabel('Loss')\n","plt.title('Training Loss Over Epochs')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"1I53vHMjIdmA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation\n","\n","Evaluate the performance of the trained neural network model on the test dataset."],"metadata":{"id":"StzzSKJDH1fC"}},{"cell_type":"code","source":["correct = 0\n","total = 0\n","\n","# Context manager under which all the operations will have `requires_grad=False`,\n","# meaning that PyTorch will not calculate or keep track of gradients.\n","# This is used because gradient computation is not needed for evaluation and saves memory and computation.\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        outputs = model(inputs)\n","\n","        # Get the index of the max log-probability\n","        pred = outputs.argmax(dim=1)\n","        correct += pred.eq(labels).sum().item()\n","\n","# Accuracy is calculated as the percentage of correct predictions over the total number of predictions.\n","accuracy = 100. * correct / len(test_loader.dataset)\n","\n","print(f\"\\nTest Set Accuracy: {correct}/{len(test_loader.dataset)} (= {accuracy:.0f}%)\\n\")"],"metadata":{"id":"bl-FhQRIH04e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"R35OlZ8eHH9c"},"execution_count":null,"outputs":[]}]}